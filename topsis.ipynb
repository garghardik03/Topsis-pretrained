{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def calculate_perplexity(generated_text):\n",
    "    input_ids = tokenizer.encode(generated_text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits\n",
    "    loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), input_ids.view(-1), reduction=\"none\")\n",
    "    perplexity = torch.exp(torch.mean(loss))\n",
    "    return perplexity.item()\n",
    "\n",
    "def calculate_fluency_score(generated_text):\n",
    "    perplexity = calculate_perplexity(generated_text)\n",
    "    return 1 / perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diversity_score(generated_text):\n",
    "    words = generated_text.split()\n",
    "    unique_words = len(set(words))\n",
    "    total_words = len(words)\n",
    "    diversity_score = unique_words / total_words\n",
    "    return diversity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "brown_bigrams = set(nltk.bigrams(brown.words()))\n",
    "\n",
    "def calculate_coherence_score(generated_text):\n",
    "    words = generated_text.split()\n",
    "    bigrams = list(zip(words[:-1], words[1:]))\n",
    "    common_bigrams = sum(bigram in brown_bigrams for bigram in bigrams)\n",
    "    coherence_score = common_bigrams / len(bigrams)\n",
    "    return coherence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text for model distilgpt2 : Once upon a time of war, the United States was the only country in the world to have a military presence. The United States was the only country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text for model gpt2 : Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text for model gpt2-medium : Once upon a time, there was a man who lived in a village called Krakow. He was a very good man, and he was very\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "\n",
    "models = [('distilgpt2', 'distilgpt2-small'),('gpt2', 'gpt2-small'),('gpt2-medium', 'gpt2-medium')]\n",
    "\n",
    "criteria = {'fluency': 0.3, 'diversity': 0.3, 'coherence': 0.2}\n",
    "\n",
    "def generate_text_and_calculate_scores(model_name, model_type):\n",
    "    generator = pipeline('text-generation', model=model_name)\n",
    "    generated_text = generator(\"Once upon a time\", max_length=30, do_sample=False, truncation=True)\n",
    "    print(\"Generated text for model\", model_name, \":\", generated_text[0]['generated_text'])\n",
    "    generated_text = generated_text[0]['generated_text']\n",
    "    fluency_score = calculate_fluency_score(generated_text)\n",
    "    diversity_score = calculate_diversity_score(generated_text)\n",
    "    coherence_score = calculate_coherence_score(generated_text)\n",
    "    return (model_name, model_type, fluency_score, diversity_score, coherence_score)\n",
    "\n",
    "data = []\n",
    "for model_name, model_type in models:\n",
    "    scores = generate_text_and_calculate_scores(model_name, model_type)\n",
    "    data.append(scores)\n",
    "\n",
    "data = np.array(data)[:, 2:].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Models (from best to worst):\n",
      "Rank 1: Model gpt2 (gpt2-small) with TOPSIS score 0.13\n",
      "Rank 2: Model distilgpt2 (distilgpt2-small) with TOPSIS score 0.41\n",
      "Rank 3: Model gpt2-medium (gpt2-medium) with TOPSIS score 0.86\n"
     ]
    }
   ],
   "source": [
    "normalized_data = data / np.sqrt((data ** 2).sum(axis=0))\n",
    "weighted_data = normalized_data * np.array(list(criteria.values()))\n",
    "ideal_solution = np.max(weighted_data, axis=0)\n",
    "anti_ideal_solution = np.min(weighted_data, axis=0)\n",
    "distance_from_ideal = np.sqrt(((weighted_data - ideal_solution) ** 2).sum(axis=1))\n",
    "distance_from_anti_ideal = np.sqrt(((weighted_data - anti_ideal_solution) ** 2).sum(axis=1))\n",
    "topsis_score = distance_from_anti_ideal / (distance_from_ideal + distance_from_anti_ideal)\n",
    "ranked_models = np.argsort(topsis_score)\n",
    "print(\"Ranked Models (from best to worst):\")\n",
    "for rank, model_idx in enumerate(ranked_models):\n",
    "    model_name, model_type = models[model_idx]\n",
    "    print(f\"Rank {rank + 1}: Model {model_name} ({model_type}) with TOPSIS score {topsis_score[model_idx]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
